## CCJS 200 - Statistics for Criminology & Criminal Justice (Spring 2024)

### Course Syllabus

* Course description (from the University catalog): Introduction to descriptive and inferential statistics, graphical techniques, and the computer analysis of criminology and criminal justice data. Basic procedures of hypothesis testing, correlation and regression analysis, and the analysis of continuous and binary dependent variables. Emphasis upon the examination of research problems and issues in criminology and criminal justice.
* Meetings: This course is scheduled to meet on Tuesdays and Thursdays from 11-12:15pm in LeFrak 2205 and I plan to hold office hours on Tuesdays from 1-2:30pm and by appointment (you can email me at rbrame@umd.edu). My office is 2139 LeFrak Hall.
* Graduate Teaching Assistants: Jae Eun (Jane) Lee (jelee18@umd.edu) and Jordan Pierce (jmpierce@umd.edu); they will each be holding their own office hours (Jordan on Monday from 1-2 and Jane on Wednesday from 1-2) and will also be responsible for overseeing discussion sections which will meet on Fridays (please see your registration information to identify your section).  Their office is 2163 LeFrak Hall
* Course-related policies: In all matters, I will follow University guidance as outlined [here](https://www.ugst.umd.edu/courserelatedpolicies.html).
* Accessibility accommodations: As a matter of University policy, I do my best to abide by all recommended accommodations. If you think you might need one or more academic accommodations, please contact the Accessibility and Disability Service Office ([link](https://ads.umd.edu)) for guidance and assistance.
* Email policy: Please refrain from sending messages to us (me, Jane, and Jordan) on ELMS. We ask that you email us directly at our UMD email addresses listed above.
* Required Textbook: Bachman, Paternoster, and Wilson (2022, 5th edition), *Statistics for Criminology and Criminal Justice*. 
* Class Notes: the class notes will be posted on this webpage.
* Letter grades: At the end of the semester, letter grades will be assigned on a 100-point scale (A+ = 97 and higher; A = 93-96; A- = 90-92; B+ = 87-89; B = 83-86; B- = 80-82; C+ = for 77-79; C = 73-76; C- = 70-72; D+ = 67-69; D = 63-66; D- = 60-62; and F = any grade less than 60). All numeric grades (including the final numeric grade in the class at the end of the semester) will be rounded off to the nearest 1 point (for example, a 78.5 would be rounded to a 79 and a 78.4 would be rounded to a 78).
* Numeric grades in this class will be based on 3 in-class exams and 3 out-of-class assignments and will all be graded on a 100-point scale. The final numeric grade calculation at the end of the semester will be: 0.25 x Exam 1 + 0.25 x Exam 2 + 0.25 x Exam 3 + 0.25 x Average Assignment Grade. (Addendum: there will also be a second formula: 0.2 x Exam 1 + 0.2 x Exam 2 + 0.2 x Exam 3 + 0.4 x Average Assignment Grade. At the end of the semester, I will calculate your grade using both approaches and I will assign your grade based on the formula that gives you the higher grade). Any formulas needed for exams will be printed on the exam.
* Key dates: (1) first class day - Thursday 1/25; (2) spring break - 3/19-3/21; (3) last day of class - Thursday 5/9; and (4) scheduled final exam period - Saturday May 11th from 8-10am.
* Attendance expectations: My expectation is that you will attend all of the class and discussion sessions (unless you are sick or have some other good reason not to attend). If you have to miss a class or a discussion section, I encourage you to work with other people in the class to get caught up on your notes and contact me or the TA's if you ever need clarification or assistance. If you have to miss an exam, we will follow the notification and make-up policies specified by the University.
* Statistical software: As noted in the catalog description, there must be a computer component to this course. We will be using R, which is free, easy to get, and works on both Windows and Mac computers.

#### Answers to Important Questions (still part of the syllabus)

* Why do I have to take this course?

The University of Maryland has a General Education curriculum [(link)](https://gened.umd.edu/node/35) which all baccalaureate-level degree recipients must complete. Part of this curriculum is called Fundamental Studies and successful completion of this course (CCJS 200) satisfies the Fundamental Studies Analytic Reasoning (FSAR) requirement. So, when you see FSAR labels associated with this course, that is what those labels refer to. The Analytic Reasoning requirement expresses a belief among the University faculty that analytic reasoning is an essential part of what it means to have received a liberal arts education.

* What does UMD expect us to cover in a class that satisfies the University's analytic reasoning requirement?

According to the Gen-Ed [website](https://gened.umd.edu/students/four-categories/fundamental-studies), "[c]courses in Analytic Reasoning foster a student's ability to use mathematical or formal methods or structured protocols and patterns of reasoning to examine problems or issues by evaluating evidence, examining proofs, analyzing relationships between variables, developing arguments, and drawing conclusions appropriately. Courses in this category advance and build upon the skills that students develop in Fundamental Mathematics."

* Why is this course taught in the CCJS department?

The disciplinary focus of this course reflects a belief by the faculty that analytic reasoning skills can be effectively conveyed through the disciplinary lens of the student's major. The alternative would be to teach a course like this in a math or statistics department and, indeed, this is what is done at some universities. There is no right or wrong way to deliver this kind of course; what we have at Maryland reflects the views of the faculty who work here.

* I don't like math and I'm apprehensive about taking this course. Can you help me feel better about this?

It just so happens that when criminologists do research and evaluation work, they often rely on quantitative data and so it came to pass that we have this class which emphasizes "statistical analysis." Even though you will be performing some calculations and working with some numeric data in this class, the calculations you will be doing are probably best viewed as "arithmetic" (which is a subset of the field of mathematics). This is to emphasize that all of the calculating in this class will be focused on addition, substraction, multiplication, and division -- in other words, skills with which you're pretty familiar. In addition, each chapter of the textbook has practice problems at the end of the chapter with answers to the odd-numbered exercises at the back of the book. I encourage you to build confidence in your ability to solve problems by working on the exercises that coincide with the material we cover in class (please note: we won't address every single topic that is discussed in the book so you should only work on practice problems that are consistent with the class material). As preparation, you are also encouraged to work on the practice arithmetic problems in Appendix A at the back of the textbook. Your discussion sections and office hours are the appropriate venue for getting help with the practice problems. The University also offers the [Math Success Program](https://tltc.umd.edu/students/get-help-class/math-success-program) which may be helpful to you.

* What are the pre-requisites for this class?

You need to have completed CCJS 100 (Intro to Criminal Justice) *or* CCJS 105 (Intro to Criminology); *and* you need to have attained a grade of C- or better in STAT 100 (Elementary Statistics and Probability), MATH 107 (Introduction to Math Modeling and Probability), MATH 120 (Elementary Calculus I), or MATH 135 (Calculus I). Less common, but still acceptable, courses include MATH 111 or MATH 130. If you haven't met these requirements, you should *drop this class now* and return when they've been completed.

* What will I be able to do when this course is over?

These are the course learning outcomes which I have defined as your being able to: (1) explain the meaning, limitations, and calculation of commonly used statistics related to crime and criminal justice; (2) distinguish between descriptive and inferential analysis and how tools in both domains are used to advance knowledge about crime and criminal justice; (3) analyze certain kinds of quantitative criminological evidence, considering both the strengths and weaknesses of methodological approaches often used in our discipline; and (4) perform basic statistical calculations related to criminologically interesting phenomena. 

* What will the assignments and exams be like in this class?

You will have 3 out-of-class assignments in this course; each assignment will consist of substantive and computer-related applications of key concepts taught in class. You will have 1 week to complete each of the out-of-class assignments. Also, each of the assignments will be posted on this webpage and will be submitted on ELMS at or before 11:59pm on the due date (if you submit after 11:59pm on the due date, there will be a 3-point per hour deduction on your assignment grade). All assignments should be submitted in pdf format (3-point deduction for assignments not submitted in pdf format). Exams will involve pencil and paper problems and calculations. For these exams, you will need to have a calculator which can take square roots. You will be able to use your paper notes (no books, phones, or computers) on the exam but we will provide all needed formulas on the exam itself so you will not need to spend any time looking up formulas. On exam days, the full class period will be devoted to the exam. For both out-of-class assignments and exams, you must show us your work in order to receive full credit. In cases where you provide an answer without showing the underlying work (or, when applicable, the relevant computer script and output), you will only receive one-half credit. Similarly, if you get an incorrect answer to a question or problem but you show your work, you will receive partial credit for the work that was done correctly.

* What happens in discussion sections?

Jane and Jordan will be the CCJS 200 teaching assistants this semester and they will oversee the discussion sections each Friday. Both of them are well versed in the concepts and materials we are discussing this semester. During the discussion sections, Jane and Jordan will be answering lecture- and assignment-related questions; they will also go over practice problems at the end of the textbook chapters. They are a valuable resource for you this semester and I encourage you to fully engage with the discussion section each week. 

* How do I get and use R?

R is available at this [website](https://www.r-project.org). You can read about the history of R (it was originally developed at Bell Labs and used to be called S and then S+) at the Wikipedia [page](https://en.wikipedia.org/wiki/R_(programming_language)). To work with R, you will need to use a plain text editor (Notepad on Windows or TextEdit on Macs) or a program like RStudio. I will be going over the basics in class; Jane and Jordan will go over RStudio in the discussion sections. R and RStudio are both available on the OACS ([website](https://oacs.umd.edu/facilities/oacs-computer-labs)) and McKeldin PC's ([website](https://umd.app.box.com/s/6gtvcfbos7x837wlvsf2denyrvghs5w6)).

#### Course Outline

This is an aspirational course outline. I will try to stick to the schedule that is described here. That said, we may need to make some changes as we go along. If that happens, I will notify you as soon as possible.

* Thursday 1/25: Meet & Greet Day + Different kinds of research that rely on quantitative data (chapter 1, part 1)
* Week 1 - 1/30-2/2: Sampling (chapter 1, part 2)
* Week 2 - 2/6-2/9: Descriptive and inferential statistics (chapter 1, part 3)
* Assignment #1 posted on Friday 2/9; due at end of the day on Friday 2/16
* Week 3 - 2/13-2/16: Levels of measurement + dichotomies, counts, rates, and percentages (chapter 2).
* Week 4 - 2/20-2/23: Central tendency (chapter 4).
* Week 5 - 2/27 is catch-up/review day and Exam 1 will be on Thursday 2/29.
* Week 6 - 3/5-3/8: Variation and dispersion (chapter 5)
* Assignment #2 posted on Friday 3/8; due at end of the day on Friday 3/15.
* Week 7 - 3/12-3/15: Probability theory (chapter 6).
* Week 8 -  3/19-3/22: Spring break!
* Week 9 - 3/26-3/29: Point estimation and confidence intervals (chapter 7).
* Week 10 - 4/2 is catch-up/review day and Exam 2 will be on Thursday 4/4.
* Week 11 - 4/9-4/12: Identification intervals (no chapter; I will provide information)
* Assignment #3 posted on Friday 4/12; due at end of the day on Friday 4/19.
* Week 12 - 4/16-4/19: One-sample hypothesis tests (chapter 8).
* Week 13 - 4/23-4/26: Categorical data (chapter 9).
* Week 14 - 4/30-5/3: Two-sample tests (chapter 10).
* Week 15 - 5/7-5/9: Correlation/regression (chapter 12) + final exam review.
* Final exam - Saturday 5/11: 10:30am-12:30pm ([official university exam schedule](https://registrar.umd.edu/registration/register-classes/final-exams/spring-2024)).

### Lesson 1 - Thursday 1/25/24

* Assigned reading: Chapter 1.
* Slides in pdf format ([link](https://github.com/rwb/ccjs200/blob/main/gfiles/lesson1.pdf)).
* Some links mentioned in today's lesson are listed below.
* Uniform Crime Reports - let's look at [2018](https://ucr.fbi.gov/crime-in-the-u.s/2018/crime-in-the-u.s.-2018) and [2019](https://ucr.fbi.gov/crime-in-the-u.s/2019/crime-in-the-u.s.-2019/topic-pages/offenses-known-to-law-enforcement).
* National Crime Victimization Survey (previously called the National Crime Survey), most recent [report](https://bjs.ojp.gov/document/cv22.pdf). 
* 2022 Juvenile Court Data ([report](https://ojjdp.ojp.gov/publications/2022-national-report.pdf)).
* Washington Post police shootings [database](https://www.washingtonpost.com/graphics/investigations/police-shootings-database/).

### Lesson 2 - Tuesday 1/30/24

* Assigned Reading: Chapter 1 (continued).
* Update on course grading: I've decided that at the end of the semester, I will calculate your grades two different ways. The first way will assign 25% weight to each of your exams and 25% weight to the average of your assignment grades. That is what is presented in the syllabus. Second, however, I will calculate your grade assigning 20% weight to each of your exams and 40% weight to the average of your assignment grades. I will assign your final letter grade based on the higher grade out of these 2 calculations.
* Slides in pdf format ([link](https://github.com/rwb/ccjs200/blob/main/gfiles/lesson2.pdf)).
* Washington Post [article](https://www.washingtonpost.com/opinions/2023/11/08/violent-crime-data-2022-mystifying/) mentioned in class.
* Uniform Crime Reports handling of 9/11/2001 terror attack ([link](https://ucr.fbi.gov/crime-in-the-u.s/2001/toc01.pdf)).

### Lesson 3 - Thursday 2/1/24

* Assigned Reading: Chapter 1 (continued).
* Slides in pdf format ([link](https://github.com/rwb/ccjs200/blob/main/gfiles/lesson3.pdf)).
* R [website](https://www.r-project.org).
* RStudio [website](https://posit.co/download/rstudio-desktop/).
* OACS Computer Labs, bottom floor of LeFrak ([website](https://oacs.umd.edu/facilities/oacs-computer-labs)). 

### Lesson 4 - Tuesday 2/6/24

* Assigned Reading: Chapter 1 (continued).
* Note: we will finish chapter 1 this week and we will post the first assignment on Friday 2/9/24 (due on Friday 2/16/24).
* Slides in pdf format ([link](https://github.com/rwb/ccjs200/blob/main/gfiles/lesson4.pdf)).

### Lesson 5 - Thursday 2/8/24

* Assigned Reading: Chapter 1 (continued).
* Slides in pdf format ([link](https://github.com/rwb/ccjs200/blob/main/gfiles/lesson5.pdf)).
* Please note: to avoid copy/paste problems, all R code and output we have covered so far is included below.
* In the future, all R code will be posted directly on Github.

### R code for our examples so far

* Basic Calculations in R

```R
x=2
y=3
x+y
x-y
x*y
x/y
x^y
factorial(x*y)
sqrt(y^x)
```

Here is the output:

```Rout
> x=2
> y=3
> x+y
[1] 5
> x-y
[1] -1
> x*y
[1] 6
> x/y
[1] 0.6666667
> x^y
[1] 8
> factorial(x*y)
[1] 720
> sqrt(y^x)
[1] 3
>
```

* Calculating your grade example

```R
exam1=77
exam2=81
exam3=80
assignment1=93
assignment2=87
assignment3=88
average.assignment=mean(assignment1,assignment2,assignment3)
average.assignment
formula1=0.25*exam1+0.25*exam2+0.25*exam3+0.25*average.assignment
formula1
formula2=0.2*exam1+0.2*exam2+0.2*exam3+0.4*average.assignment
formula2
```

Here is the output:

```Rout
> exam1=77
> exam2=81
> exam3=80
> assignment1=93
> assignment2=87
> assignment3=88
> average.assignment=mean(assignment1,assignment2,assignment3)
> average.assignment
[1] 93
> formula1=0.25*exam1+0.25*exam2+0.25*exam3+0.25*average.assignment
> formula1
[1] 82.75
> formula2=0.2*exam1+0.2*exam2+0.2*exam3+0.4*average.assignment
> formula2
[1] 84.8
>
```

* Household Burglaries in Charlotte and Wilmington Example

```R
nburg.clt=7305
nburg.wil=1109
nburg.clt/nburg.wil
pop.clt=779541
pop.wil=106476
burgrate.clt=(nburg.clt/pop.clt)*100000
burgrate.clt
burgrate.wil=(nburg.wil/pop.wil)*100000
burgrate.wil
burgrate.clt/burgrate.wil
```

And, here are the results:

```Rout
> nburg.clt=7305
> nburg.wil=1109
> nburg.clt/nburg.wil
[1] 6.587015
> pop.clt=779541
> pop.wil=106476
> burgrate.clt=(nburg.clt/pop.clt)*100000
> burgrate.clt
[1] 937.0899
> burgrate.wil=(nburg.wil/pop.wil)*100000
> burgrate.wil
[1] 1041.549
> burgrate.clt/burgrate.wil
[1] 0.8997077
>
```

* UCR Murder Rates by Year (1994-2019) Example

```R
year=seq(from=1994,to=2019,by=1)

murders=c(23326,21606,19645,18208,16974,15522,
15586,16037,16229,16528,16137,16692,17034,
16929,16272,15241,14748,14612,14827,14196,
14249,15696,17250,17284,16214,16425)

pop=c(259177778,263487805,265472973,267764706,
269428571,272315789,283381818,286375000,
289803571,289964912,293400000,298071429,
293689655,297000000,301333333,304820000,
307250000,310893617,315468085,315466667,
323840909,320326531,325471698,326113208,
324280000,328500000)

mrate=(murders/pop)*100000

plot(x=year,y=mrate,type="l",ylim=c(0,10),
  main="UCR Murder Rates by Year (1994-2019)",
  xlab="Year (1994-2019)",
  ylab="# of Murders per 100k Population")
points(x=year,y=mrate,pch=19)
```

and here is our output:

<p align="center">
<img src="/gfiles/f1c.png" width="700px">
</p>

* Age of People Released from NC Prisons in 1978 Example

```R
# read the dataset - NC Department of Corrections FY1978 Releases

age=c(rep(16,19),rep(17,161),rep(18,492),rep(19,480),rep(20,624),
  rep(21,599),rep(22,580),rep(23,468),rep(24,537),rep(25,443),rep(26,432),
  rep(27,338),rep(28,415),rep(29,292),rep(30,324),rep(31,254),rep(32,234),
  rep(33,179),rep(34,187),rep(35,167),rep(36,177),rep(37,132),rep(38,152),
  rep(39,117),rep(40,119),rep(41,93),rep(42,113),rep(43,102),rep(44,85),
  rep(45,75),rep(46,90),rep(47,72),rep(48,86),rep(49,62),rep(50,78),
  rep(51,61),rep(52,57),rep(53,50),rep(54,44),rep(55,49),rep(56,55),
  rep(57,34),rep(58,34),rep(59,25),rep(60,21),rep(61,18),rep(62,19),
  rep(63,11),rep(64,16),rep(65,7),rep(66,5),rep(67,13),rep(68,5),rep(69,3),
  rep(70,1),rep(71,3),rep(72,5),rep(73,3),rep(74,4),rep(75,2),rep(77,2),rep(78,2))
n=length(age)
n

# part 1: create a chart

barplot(table(age),
  xlab="Age (in years) at Time of Release",
  ylab="Number of People",
 main="Age at Release from Prison (1978 NCDOC)") 

# part 2: average age at release for the population

mean(age)
```

* Here is the output:

```Rout
> # read the dataset - NC Department of Corrections FY1978 Releases
> 
> age=c(rep(16,19),rep(17,161),rep(18,492),rep(19,480),rep(20,624),
+   rep(21,599),rep(22,580),rep(23,468),rep(24,537),rep(25,443),rep(26,432),
+   rep(27,338),rep(28,415),rep(29,292),rep(30,324),rep(31,254),rep(32,234),
+   rep(33,179),rep(34,187),rep(35,167),rep(36,177),rep(37,132),rep(38,152),
+   rep(39,117),rep(40,119),rep(41,93),rep(42,113),rep(43,102),rep(44,85),
+   rep(45,75),rep(46,90),rep(47,72),rep(48,86),rep(49,62),rep(50,78),
+   rep(51,61),rep(52,57),rep(53,50),rep(54,44),rep(55,49),rep(56,55),
+   rep(57,34),rep(58,34),rep(59,25),rep(60,21),rep(61,18),rep(62,19),
+   rep(63,11),rep(64,16),rep(65,7),rep(66,5),rep(67,13),rep(68,5),rep(69,3),
+   rep(70,1),rep(71,3),rep(72,5),rep(73,3),rep(74,4),rep(75,2),rep(77,2),rep(78,2))
> n=length(age)
> n
[1] 9327
> 
> # part 1: create a chart
> 
> barplot(table(age),
+   xlab="Age (in years) at Time of Release",
+   ylab="Number of People",
+  main="Age at Release from Prison (1978 NCDOC)") 
> 
> # part 2: average age at release for the population
> 
> mean(age)
[1] 29.32787
>
```

* Now, let's draw a simple random sample from the population, calculate the sample average, and draw a sample barplot.

```R
# part 3: study a simple random sample

s=sample(1:n,size=100,replace=T)
sample.age=age[s]
mean(sample.age)

# part 4: create a chart showing the ages in the sample

barplot(table(sample.age),
  xlab="Age (in years) at Time of Release",
  ylab="Number of People",
 main="Age at Release for Random Sample")
```

* Here is the output for the random sample:

```Rout
> # part 3: study a simple random sample
> 
> s=sample(1:n,size=100,replace=T)
> sample.age=age[s]
> mean(sample.age)
[1] 31.18
> 
> # part 4: create a chart showing the ages in the sample
> 
> barplot(table(sample.age),
+   xlab="Age (in years) at Time of Release",
+   ylab="Number of People",
+  main="Age at Release for Random Sample")
>
```

<p align="center">
<img src="/gfiles/f3.png" width="700px">
</p>

### Assignment #1 - Due 11:59pm Friday 2/16/24

* Instructions: Please complete each of the 3 parts listed below. You are allowed to discuss your work with other students and the TA's but the work you submit should be your own work. If you have a question you want to submit in writing, we ask that you post it to the discussion board rather than email. If you do not want to be identified as a questioner by the rest of the class, you may send your question to one of us and we will respond (during business hours). Please note that all questions and answers that we judge to be of interest to the entire class will also be posted on the discussion board. This assignment will be due in pdf format on ELMS by 11:59pm on Friday February 16, 2024.

* Part 1: In 2011, Greensboro police recorded 3,279 household burglaries while Durham police recorded almost the same number -- 3,283. According to the state population database, Greensboro had 263,279 residents in 2011 while Durham had 222,978. Based on this information calculate the number of household burglaries per 100,000 population in each city and identify the city with the higher household burglary rate. Write a sentence where you describe what you learned from this comparison.

* Part 2: The table gives the total number of homicides in the U.S. for each year from 1994-2019 based on death certificate data from the National Vital Statistics System (NVSS). Using these data construct a figure showing the yearly homicide rates per 100,000 population (similar to the in-class example where we calculated homicide rates using the UCR data). Comment on which series tends to have higher levels -- the one based on UCR data or this one based on the CDC death certificate data.

<p align="center">
<img src="/gfiles/homicide-table.png" width="700px">
</p>

* Part 3: In practice exercise 3, we looked at the age distribution of North Carolina prison releasees in 1978. For this task, you will examine a similar age distribution for the 9,549 people who were released from prison in 1980. The R code for the ages are printed below:

```R
age=c(rep(15,1),rep(16,20),rep(17,224),rep(18,504),rep(19,472),rep(20,626),
  rep(21,517),rep(22,601),rep(23,516),rep(24,565),rep(25,407),rep(26,495),
  rep(27,302),rep(28,397),rep(29,291),rep(30,298),rep(31,261),rep(32,330),
  rep(33,224),rep(34,231),rep(35,163),rep(36,194),rep(37,157),rep(38,149),
  rep(39,125),rep(40,129),rep(41,116),rep(42,100),rep(43,88),rep(44,105),
  rep(45,88),rep(46,80),rep(47,72),rep(48,60),rep(49,68),rep(50,67),
  rep(51,64),rep(52,50),rep(53,47),rep(54,51),rep(55,47),rep(56,42),
  rep(57,28),rep(58,39),rep(59,12),rep(60,29),rep(61,12),rep(62,13),
  rep(63,8),rep(64,19),rep(65,12),rep(66,9),rep(67,2),rep(68,5),rep(69,3),
  rep(70,6),rep(71,1),rep(73,2),rep(74,2),rep(75,1),rep(77,1),rep(79,1))
```

With these ages, you should construct a barplot showing the age distribution for the population of people released from prison in 1980. Next, you should calculate the average age at the time of release for the 1980 cohort. Then, you are asked to draw a simple random sample of 300 people from this population. Create a barplot showing the age distribution for your random sample of people released from prison. Calculate the average age at the time of release for the sample. Compare your population average from 1980 to the population average in 1978. Which one is greater? Now, draw a simple random sample of 300 people from your 1978 population. Compare your sample average from 1978 to your sample average from 1980. Finally, compare the conclusions you draw for the 2 populations to the conclusions you draw from your 2 random samples. *Note*: when I say "compare" I mean show the numeric answers and write a sentence interpreting your results.

### Lesson 6 - Tuesday 2/13/24

1. Assigned Reading - Chapter 2 of textbook.
2. Definition of a variable: a quantity that can take on different values.
3. Definition of a constant: a quantity that can take on only a single value.
4. In descriptive research, we seek to describe the *location* and the *distribution* of scientifically interesting variables.
5. In causal research, we seek to *explain* the variation that is observed in a scientifically interesting variable.
6. In evaluation research, we might do both.
7. A *sample space* is a list or set of all possible outcomes (values that a variable could have).
8. The outcomes in a sample space must be mutually exclusive and exhaustive.
9. A *frequency distribution* tells us the number of times that a variable takes on each value in the sample space (slightly different from the book's definition; page 25).
10. Variables can be categorized by *level of measurement*.
11. The most basic distinction is between *qualitative* (categorical) and *quantitative* (continuous) variables.
12. Within the category of qualitative variables, there are *nominal* (unordered categories) and *ordinal* (small number of ordered categories; the distance between any of the categories is not well defined).
13. Within the category of quantitative variables, there are *interval* (many ordered categories but no true zero) and *ratio* (continuous with a true zero) variables.
14. Examples of qualitative variables: (1) *nominal*: type of robbery (commercial robbery, personal robbery, purse snatching); and (2) *ordinal*: level of injury in a victimization (no injury, injured but not hospitalized, injured and hospitalized but survived, and fatal injury).
15. Examples of quantitative variables: (1) interval: an offense gravity score for criminal sentencing; and (2) ratio: amount of money stolen from the victim in a robbery.
16. In criminology, we are typically concerned with *random variables* where we accept the idea that each outcome in the sample space (for qualitative variables) or interval of the sample space (for quantitative variables) occurs with a probability, *p*. So, if there are two outcomes in the sample space it makes sense to think about outcome 1 occurring with probability, *p<sub>1</sub>*, and outcome 2 occurring with probability, *p<sub>2</sub>*.
17. In this class, we think about *p* in terms of relative frequencies (note: a relative frequency is the number of times an event occurs divided by the total number of times it could have occurred).
19. Example: in a sample of 1,000 prison releasees, we find that 675 people failed (recidivism) while the remaining 325 people did not fail. In this example, *failure* is a random variable where the sample space has two outcomes, "failed" or "did not fail". Thus, *p*(failed) = 675/1000 = 0.675 and *p*(did not fail) = 325/1000 = 0.325. 
20. Note that *p* must always be a number in the interval [0,1]. The bracket [] notation means that the numbers 0 and 1 are included in the interval so that 0 ≤ *p* ≤ 1.
21. When *p* = 0, we say the outcome cannot occur; symmetrically, when *p* = 1, we say that the outcome must occur.
22. The sum of the probabilities of the outcomes in the sample space is 1.0 (for quantitative variables, the sum is an integral).
23. Sometimes, we consider the probabilities of each of the outcomes in a sample space together; the set of probabilities corresponding to the outcomes in a sample space is called a *probability distribution*.
24. Example: we interview a sample of people exiting prison to measure their risk of recidivism. Based on the answers, we divide the respondents (people who participate in a survey) into 3 groups: low, medium, and high risk.

| Outcomes    | N = |
| :---------- | --: |
| Low Risk   | 773        |
| Medium Risk  | 242        |
| High Risk | 108        |
| Total | 1123 |

Questions about this example: (1) what is the level of measurement?: (2) what are the outcomes in the sample space?; (3) what are the relative frequencies for each of the outcomes?; (4) what is the probability that someone drawn at random from this sample is high risk?;

### Lesson 7 - Thursday 2/15/24

1. Reminder: first assignments are due on ELMS by 11:59pm on Friday 2/16/24; after that point, late submission deductions begin. You must submit your assignment in a pdf file.
2. Assigned reading, Chapter 2; you can also go ahead and begin looking at chapter 4.
3. Recall from our last class that we presented a taxonomy of different kinds of variables corresponding to their levels of measurement. Here is a summary:

<p align="center">
<img src="/gfiles/vartypes.png" width="600px">
</p>

4. Dichotomous/binary variables: 2-category variables that usually measure either the presence or absence of an attribute, experience, or intervention. This means the sample space is usually written down in terms of the presence or absence of a condition. Examples: (1) whether someone gets arrested; (2) whether someone has been a crime victim; (3) whether a study participant has been assigned to a treatment or control group; (4) whether a state has the death penalty; and (5) whether a U.S. county experienced any homicides in a given year. With dichotomous or binary variables, we are often interested in the fraction or proportion of cases that/who are in each category of the sample space.
   
5. Event count variables: these are variables that measure the number of times that something happens, so the sample space is comprised of positive integers (whole numbers). Event counts are qualitative variables in the sense that the sample space is only comprised of integers (fractional values of counts for a specific unit don't make sense). But it does make sense to think about averages of counts. Statements like 2x is twice the number of events as 1x (where x is a counted variable value) are reasonable because counts can have a true zero. Event counts are often converted into rates, where we divide the count by some amount of time or by some number of people (to get the number of events in a particular period of time (say, 1 year) or the number of events per number of people in the population (say, 100,000 people)).
   
6. Criminologists have to be careful in their analysis of binary and count variables because they present special analysis issues.

7. In scientific work, we distinguish between independent variables (potential causes) and dependent or outcome variables. Example: Suppose we hypothesize that poor school performance is a cause of juvenile delinquency and we collect data to test this hypothesis. In our study, we would characterize "school performance" as our independent variable and "juvenile delinquency" as our outcome or dependent variable.

8. It is also essential that we clearly define the *unit of analysis* in any empirical study. The number of cases in the study can help us think about this.
   - Example 1: from your assignment that is due tomorrow, the unit of analysis in the NC age at time of release from prison study, the unit of analysis is each individual person leaving a NC prison in the year 1980 (N = 9,549).
   - Example 2: also, from your assignment this week, the unit of analysis in the CDC homicide study is the year (1994-2019; N = 26).
   - Whenever you encounter a study you should try to clearly identify the unit of analysis (a description of the particular cases being studied).

9. Two issues that often arise in criminology studies: (1) the amount of change that occurs from one time to the next; and (2) the variation in how often something happens between different groups.

10. Example 1: In 2018, the city of Pittsburgh reported that there were 57 murders known to the police ([link](https://ucr.fbi.gov/crime-in-the-u.s/2018/crime-in-the-u.s.-2018/tables/table-8/table-8-state-cuts/pennsylvania.xls)). By comparison, the city of Philadelphia reported 351 murders. These are both counts. Yet it is problematic to compare them directly in terms of a discussion about which city experiences a higher incidence of murders. The problem is the 2 cities have vastly different population sizes: Philadelphia's population in 2018 was estimated to be 1,586,916 while Pittsburgh's population was estimated to be 302,544. Surely, this should be taken into consideration. Here is how we could do it:

- Pittsburgh: # of murders per 100,000 population = (57/302544)*100000 = 18.8
- Philadelphia: # of murders per 100,000 population = (351/1586916)*100000 = 22.1

Based on this comparison, we can say that Philadelphia's murder rate is higher than Pittsburgh's but it is a dramatically different story than a comparison of 351 to 57.

11. Example 2: Each year, the National Crime Victimization Survey (NCVS) publishes a national personal robbery victimization rate per 1,000 persons covering the entire nation. For the year [2018](https://bjs.ojp.gov/content/pub/pdf/cv21.pdf) (go to Table 1 on page 2 to see the data), the estimated number of robberies was 573,100 and the rate was estimated to be 2.1 robberies per 1,000 persons. For 2019, the estimated number of robberies was 534,420 while the rate was estimated to be 1.9 robberies per 1,000 population. Using both the count and the rate, we would infer that robberies dropped. However, we can quantify both drops by using the *percent change* statistic. To get the percent change, we have to decide which year will be the base year and which year will be the comparison year. When we are comparing years, we usually say that the first year is the base year and the second year is the comparison year. Here is our worked example:

- Base year estimated number of robberies - 2018: 573100
- Comparison year estimate number of robberies - 2019: 534420
- Percent change statistic: ((534420-573100)/573100)*100 = -6.7%

- Base year estimated number of robberies per 1000 persons - 2018: 2.1
- Comparison year estimated number of robberies per 1000 persons - 2019: 1.9
- Percent change statistic: ((1.9-2.1)/2.1)*100 = -9.5%

- Formula: ((comparison year - base year)/base year)*100

Comparing the raw number of estimated robberies from one year to the next is less meaningful than comparing the rates because the nation's estimated population size did not stay the same from one year to the next.

12. Example 3: For several decades, the Bureau of Justice Statistics (BJS) has carefully studied recidivism patterns for various groups of state prison releasees (you can see the collection [here](https://bjs.ojp.gov/data-collection/recidivism-state-prisoners#1-0)). Some relatively recent data appear in this [document](https://bjs.ojp.gov/BJS_PUB/rpr24s0810yfup0818/Web%20content/508%20compliant%20PDFs). Suppose we want to measure the fraction of people who were released from prison who are in each of 3 age groups. Here is our data:

| Age at Release from Prison    | N = | # Rearrested |
| :---------- | --: | --:|
| Age ≤ 24    | 62,700 | 47,000 |
| Age 25-39   | 206,000  | 141,500 |
| Age ≥ 40    | 140,600 | 82,400 |
| Total        | 409,300 | 270,900 |

- Based on this information, we could calculate the fraction or proportion of people who were in the youngest age group using the proportion formula on page 36:
- Proportion formula = Number of people in subset of sample / Total number of people in sample
- Calculation: 62700/409300 = 0.153
- Proportions can also be called *relative frequencies* (see top of page 37).
- Now, we often want to convert proportions into percentages which we can do by using the proportion to percent conversion formula on page 37.
- Proportion to percent conversion formula: Estimated proportion x 100
- Application: 0.153*100 = 15.3%
- Convince yourself that if you calculate the proportions and percentages for the other 2 age groups that all 3 of them will add up to 1 (proportions) and 100 (percents).
- *Note*: once you have calculated the proportion of people who are in each age group, you have the *relative frequency distribution*.

### Lesson 8 - Tuesday 2/20/24

* Reminder: first exam is scheduled for Thursday 2/29/24.
* The best way to prepare for the exam is to work on practice problems at the end of each chapter.
* I will post some additional practice problems today.
* Recall these data from the Bureau of Justice Statistics that we discussed last week:

| Age at Release from Prison    | N = | # Rearrested |
| :---------- | --: | --:|
| Age ≤ 24    | 62,700 | 47,000 |
| Age 25-39   | 206,000  | 141,500 |
| Age ≥ 40    | 140,600 | 82,400 |
| Total        | 409,300 | 270,900 |

* Last week, we calculated the proportion or percent of the sample in each age group.
* I also asked you as an exercise to convince yourself that the proportions (relative frequencies) would add up to 1 across the age categories:

```
62700/409300 = 0.153
206000/409300 = 0.503
140600/409300 = 0.344

0.153+0.503+0.344 = 1.0
```

* We can also convert these proportions into percentages:

```
0.153 x 100 = 15.3%
0.503 x 100 = 50.3%
0.344 x 100 = 34.4%

15.3+50.3+34.4 = 100.0%
```
* We can think about these proportions in terms of *probabilities*. Here is an example:

```
The probability that a person in this sample is in the 24 and younger age group is 62700/409300 = 0.153.
The probability that a person in this sample is in the 25-39 age group is 206000/409300 = 0.503.
The probability that a person in this sample is in the 40 and older age group is 140600/409300 = 0.344.
```

* We can carry out certain arithmetic operations with these probabilities.
* For example, we might want to know the probability that someone drawn at random from the sample is age 39 or younger. This is equivalent to saying "what is the probability that someone drawn at random from this sample is in the 24 and younger age group *or* the 25-39 age group?" To perform this calculation, we can add the probabilities for the first 2 groups together to get:

```
p(person in this sample is 39 or younger) = 62700/409300 + 206000/409300 = 0.153 + 0.503 = 0.656
```

* Binary/dichotomous variable example: calculating *recidivism rates*. Let's consider the BJS data (above) as a basis for calculating a recidivism rate statistic. There are different ways to calculate recidivism rates. For purposes of this class, we will define the concept of a recidivism rate as the proportion or fraction of people who are apprehended for committing new crimes within a well-defined period of time. Considering the BJS data, we have a total of 409,300 people who were in the sample of prison releasees (i.e., people who were at risk for recidivism). Within this sample, a total of 270,900 people were arrested for a new crime within 3 years of their release date. Thus, the recidivism rate calculation is:

```
Recidivism rate = # of people who failed / total # of people at risk = 270900/409300 = 0.662 or 66.2%
or equivalently,
p(recidivism) = 270900/409300 = 0.662
```
An equivalent way to think about this statistic is in probability terms. So, the probability that someone drawn at random from this sample was rearrested within 3 years is 0.662 or 66.2%. 

* We can also calculate the recidivism rate for the people in the youngest age group (24 and younger). For this calculation, we see that there were 62,700 people at risk for recidivism within this group. Among these people, a subset of 47,000 were arrested for a new crime within 3 years time. The recidvism rate for this group is:

```
Recidivism rate for people 24 and younger = 47000/62700 = 0.750 or 75.0%
```

* We can use 0/1 codes to calculate the mean of a binary/dichotomous variable. At the same time, we can introduce the concept of an arithmetic average or mean. Here is an example:
```
- Say we are in a state that has the death penalty and we want to look at the last 6 executions.
- In each of the 6 cases, we code the case 1 if the person had been on death row for more than 10 years
- We code the case 0 if the person had been on death row for less than 10 years.
- Based on this rule, 4 of our 6 cases had been on death row for more than 10 years so they receive a code of 1
- The other 2 had been on death row for less than 10 years so they receive a code of 0.
- If we add up our 0's and 1's, we get the following: 1+1+1+1+0+0 = 4.
- The 4 represents the sum of the scores.
- Since there are 6 cases, the number of scores is 6.
- The average or the mean is the sum of the scores divided by the number of scores.
- Calculation: 4/6 = 2/3 or about 0.667.
- The mean of the 0's and 1's corresponds to the proportion or fraction of cases that have a code of 1.
- This mean is also the fraction of people who spent more than 10 years on death row before execution.
```
* Practice Question #1 (PQ1): Suppose you are asked to indicate your level of agreement with the following statement: "I feel safe walking alone in my neighborhood at night." The response scale has the following options: (1) strongly agree; (2) agree; (3) disagree; (4) strongly disagree. What is the level of measurement for this variable?
```
a. nominal
b. ordinal
c. interval
d. ratio
```

Correct answer: b. ordinal

* PQ2: The city of Mount Holly has 172,000 residents. During 2023, there were 47 residential burglaries. Calculate the number of residential burglaries per 100,000 population (rounded off to the nearest 1st decimal place).
```
a. 18.2
b. 24.3
c. 27.3
d. 33.7
```

Correct answer: c. 27.3

* PQ3: Suppose that in 2022, the city of Mount Holly had 170,000 residents and 45 residential burglaries. Calculate the percentage change in the residential burglary rates for 2023 compared to 2022 (do your arithmetic out to the first decimal place).
```
a. 3 percent decrease
b. 1 percent increase
c. 3 percent increase
d. 7 percent increase
```

Correct answer: c. 3 percent increase

* PQ4: what is the probability that someone drawn at random from the prisoner releasee sample is 25 years old or older? (calculate probabilities out to 3 decimal places).
```
a. 0.537
b. 0.401
c. 0.847
d. none of the above are correct
```

Correct answer: c. 0.847

* PQ5: Calculate recidivism rates for the each of the age groups in the BJS data above. Which of the 3 groups has the highest recidivism rate?
```
a. 24 and younger
b. 25-39
c. 40 and over
```

Correct answer: a. 24 and younger

### Lesson 9 - Thursday 2/22/24

* Reminder: first exam is scheduled for Thursday 2/29/24
* Today we are going to start by reviewing the use and interpretation of binary/dichotomous variables and percent change statistics.
* We will then work on a few more practice problems.
* Finally, we transition to Chapter 4.
* Slides [linked here](https://github.com/rwb/ccjs200/blob/main/gfiles/lesson9.pdf)

### Lesson 10 - Tuesday 2/27/24

* Reminder: first exam is scheduled for Thursday 2/29/24
* Today we are going to work on some more practice problems
* We will also review for the first exam
* The exam will cover chapters 1, 2, and 4.
* Slides [linked here](https://github.com/rwb/ccjs200/blob/main/gfiles/lesson10.pdf)
* Draft Formula Sheet [linked here](https://github.com/rwb/ccjs200/blob/main/gfiles/formulas-exam1.pdf)

### Lesson 11 - Tuesday 3/5/24

* Exam 2 starts here.
* Assigned reading: Chapter 5 (measures of dispersion)
* Slides [linked here](https://github.com/rwb/ccjs200/blob/main/gfiles/lesson11.pdf)
* Here is some new R code.
* Suppose I give you the following dataset comprised of the last 300 jail sentences (in months) handed down by the county court. We denote the jail/prison sentence lengths by the variable, *x*.

```R
x = c(rep(1,5),rep(2,29),rep(3,51),rep(4,61),
  rep(5,61),rep(6,44),rep(7,22),rep(8,17),rep(9,5),rep(10,4),11)
table(x)
```

* This gives us the following output:

```Rout
> x = c(rep(1,5),rep(2,29),rep(3,51),rep(4,61),
+   rep(5,61),rep(6,44),rep(7,22),rep(8,17),rep(9,5),rep(10,4),11)
> table(x)
x
 1  2  3  4  5  6  7  8  9 10 11 
 5 29 51 61 61 44 22 17  5  4  1 
>
```

* Next, we create a barplot giving us a visual representation of this distribution:

```R
barplot(table(x),
  main="Distribution of Jail Sentences",
  ylab="Number of People",
  xlab="Number of Months Sentenced to Jail")
```
* Here is the output:

<p align="center">
<img src="/gfiles/jailtable.png" width="700px">
</p>

* Calculate the mean, median, minimum, maximum, and range of this distribution.

```R
mean(x)
median(x)
min(x)
max(x)
max(x)-min(x)
```

* Here are the results:

```Rout
> mean(x)
[1] 4.716667
> median(x)
[1] 5
> min(x)
[1] 1
> max(x)
[1] 11
> max(x)-min(x)
[1] 10
>
```

* Now, let's look at a similar dataset from the next-door neighbor county. We will call the sentence length variable in this dataset, *y*

```R
y = c(1,rep(2,7),rep(3,28),rep(4,45),rep(5,55),rep(6,57),
  rep(7,39),rep(8,28),rep(9,22),rep(10,12),rep(11,3),rep(12,3))
table(y)
```

and here is the output:

```Rout
> y = c(1,rep(2,7),rep(3,28),rep(4,45),rep(5,55),rep(6,57),
+   rep(7,39),rep(8,28),rep(9,22),rep(10,12),rep(11,3),rep(12,3))
> table(y)
y
 1  2  3  4  5  6  7  8  9 10 11 12 
 1  7 28 45 55 57 39 28 22 12  3  3 
>
```

* As before, we can calculate some descriptive information about this variable:

```R
mean(y)
median(y)
min(y)
max(y)
max(y)-min(y)
```

which gives us the following output:

```Rout
 mean(y)
[1] 5.933333
> median(y)
[1] 6
> min(y)
[1] 1
> max(y)
[1] 12
> max(y)-min(y)
[1] 11
>
```

* How do these numbers compare to those for the first county?
* Let's create a side-by-side chart for the 2 distributions:

```R
par(mfrow=c(1,2))
barplot(table(x),
  main="Distribution of Jail Sentences in County #1",
  ylab="Number of People",
  xlab="Number of Months Sentenced to Jail")
barplot(table(y),
  main="Distribution of Jail Sentences in County #2",
  ylab="Number of People",
  xlab="Number of Months Sentenced to Jail")
```

* Be sure to save this chart so you can look at it again.
* Here are the 2 charts together:

<p align="center">
<img src="/gfiles/jailtable2.png" width="800px">
</p>

* There are a number of ways we could compare these 2 charts: (1) skewness; (2) unimodality; (3) range of values; and (4) mean > median or median > mean.
* One thing that becomes apparent is that comparing two barcharts side-by-side is not the easiest task in the world. Is there something better?
* Answer: Yes!
* Introducing the boxplot:

```R
boxplot(x,y,
  main="Distribution of Jail Sentences by County",
  xlab="",
  ylab="Jail Sentence Length (in months)",
  xaxt="n")
axis(side=1,at=1:2,c("County 1","County 2"))
```

and this code gives us the following chart:

<p align="center">
<img src="/gfiles/jailtable3.png" width="800px">
</p>

* There is some complexity to these plots but they are useful for comparing distributions.

### Lesson 12 - Thursday 3/7/24

* Assigned reading: finish Chapter 5 (measures of dispersion)
* Reminder: Assignment #2 will be distributed Friday 3/8/24 at 9am; it will be due at 11:59pm on Monday 3/18/24.
* Slides [linked here](https://github.com/rwb/ccjs200/blob/main/gfiles/lesson12.pdf)
* A point about data entry in R:

```R
z = c(1,1,1,2,2,3,3,3,3)
table(z)
z = c(rep(1,3),rep(2,2),rep(3,4))
table(z)
```
which gives us this:
```Rout
> z = c(1,1,1,2,2,3,3,3,3)
> table(z)
z
1 2 3 
3 2 4 
> z = c(rep(1,3),rep(2,2),rep(3,4))
> table(z)
z
1 2 3 
3 2 4 
>
```

* Recall the jail sentence data (300 cases) from County Court #1 which we examined last time:

```R
x = c(rep(1,5),rep(2,29),rep(3,51),rep(4,61),
  rep(5,61),rep(6,44),rep(7,22),rep(8,17),rep(9,5),rep(10,4),11)
table(x)
```

Here is the frequency table:

```Rout
> x = c(rep(1,5),rep(2,29),rep(3,51),rep(4,61),
+   rep(5,61),rep(6,44),rep(7,22),rep(8,17),rep(9,5),rep(10,4),11)
> table(x)
x
 1  2  3  4  5  6  7  8  9 10 11 
 5 29 51 61 61 44 22 17  5  4  1 
>
```

* What is the range for this dataset?

```R
max(x)
min(x)
max(x)-min(x)
```

* Here is the output:

```Rout
> max(x)
[1] 11
> min(x)
[1] 1
> max(x)-min(x)
[1] 10
>
```

* Next, let's consider the inter-quartile range (IQR):

```R
quantile(x,0.75)
quantile(x,0.25)
quantile(x,0.75)-quantile(x,0.25)
```

* Here is the output:

```Rout
> quantile(x,0.75)
75% 
  6 
> quantile(x,0.25)
25% 
  3 
> quantile(x,0.75)-quantile(x,0.25)
75% 
  3 
>
```

* Note: this underscores that the IQR will generally be a smaller number than the range.
* Next, we calculate the mean and standard deviation for county 1:

```R
nx=length(x)
nx
meanx=sum(x)/nx
meanx
mean(x)
xmxsq=(x-meanx)^2
sdx=sqrt(1/(nx-1)*sum(xmxsq))
sdx
sd(x)
```
* Here is our output:

```Rout
> nx=length(x)
> nx
[1] 300
> meanx=sum(x)/nx
> meanx
[1] 4.716667
> mean(x)
[1] 4.716667
> xmxsq=(x-meanx)^2
> sdx=sqrt(1/(nx-1)*sum(xmxsq))
> sdx
[1] 1.903101
> sd(x)
[1] 1.903101
>
```

* Now, let's compare the range and the IQR for the two county courts we discussed last time. We begin by reading in the 2 datasets.

```
x = c(rep(1,5),rep(2,29),rep(3,51),rep(4,61),
  rep(5,61),rep(6,44),rep(7,22),rep(8,17),rep(9,5),rep(10,4),11)
table(x)
y = c(1,rep(2,7),rep(3,28),rep(4,45),rep(5,55),rep(6,57),
  rep(7,39),rep(8,28),rep(9,22),rep(10,12),rep(11,3),rep(12,3))
table(y)
```

* Here is the output:

```Rout
> x = c(rep(1,5),rep(2,29),rep(3,51),rep(4,61),
+   rep(5,61),rep(6,44),rep(7,22),rep(8,17),rep(9,5),rep(10,4),11)
> table(x)
x
 1  2  3  4  5  6  7  8  9 10 11 
 5 29 51 61 61 44 22 17  5  4  1 
> y = c(1,rep(2,7),rep(3,28),rep(4,45),rep(5,55),rep(6,57),
+   rep(7,39),rep(8,28),rep(9,22),rep(10,12),rep(11,3),rep(12,3))
> table(y)
y
 1  2  3  4  5  6  7  8  9 10 11 12 
 1  7 28 45 55 57 39 28 22 12  3  3 
>
```

* Here are the calculations for the range and IQR for both datasets:

```r
range.x=max(x)-min(x)
range.x
range.y=max(y)-min(y)
range.y

iqr.x=quantile(x,0.75)-quantile(x,0.25)
iqr.x
iqr.y=quantile(y,0.75)-quantile(y,0.25)
iqr.y
```

* Here is our output:

```Rout
> range.x=max(x)-min(x)
> range.x
[1] 10
> range.y=max(y)-min(y)
> range.y
[1] 11
> 
> iqr.x=quantile(x,0.75)-quantile(x,0.25)
> iqr.x
75% 
  3 
> iqr.y=quantile(y,0.75)-quantile(y,0.25)
> iqr.y
75% 
  3 
>
```
* So, these 2 distributions have different ranges but the same IQR. This suggests that there may be some differences in the extreme values of the 2 distributions. Note that differences between IQR's and ranges don't imply anything about differences between the means or medians of the 2 variables.
* Next, we compare the means and standard deviations of the 2 distributions:

```R
mean(x)
sd(x)
mean(y)
sd(y)
```

* Here are our results:

```R
> mean(x)
[1] 4.716667
> sd(x)
[1] 1.903101
> mean(y)
[1] 5.933333
> sd(y)
[1] 2.12224
>
```

* These results suggest that the mean sentence length in county 2 (y) is greater than the mean sentence length in county 1 (x). In addition, the distribution of sentence lengths in county 2 is a little more dispersed than it is in county 1. Since the distributions have some skew (review our boxplot from last time), it would also be useful to look at the medians of the 2 distributions:

```R
median(x)
median(y)
```

with the following results:

```Rout
> median(x)
[1] 5
> median(y)
[1] 6
>
```

which confirm that the median for county 2 is higher than the median for county 1. We complete the analysis by examining the boxplot:

```R
boxplot(x,y,
  main="Distribution of Jail Sentences by County",
  xlab="",
  ylab="Jail Sentence Length (in months)",
  xaxt="n")
axis(side=1,at=1:2,c("County 1","County 2"))
```

and this code gives us the following chart:

<p align="center">
<img src="/gfiles/box2.png" width="700px">
</p>

### Assignment 2 

*Note* Due to this being posted late (10:15am on Friday), I am extending the submission deadline for this assignment to Monday March 18th at 11:59pm.

Data description: Consider the problem of how long (in days) people sentenced to prison have to wait for drug treatment when they have a documented need. Suppose we have measures of this variable for samples of people from 2 different prisons and we want to compare them. We denote prison 1 by p1 and prison 2 by p2. Here is the data you should enter into R to get started:

```Rout
p1 = c(rep(14,4),rep(15,5),rep(16,10),rep(17,11),rep(18,31),
  rep(19,36),rep(20,57),rep(21,51),rep(22,66),rep(23,44),
  rep(24,32),rep(25,25),rep(26,10),rep(27,2),rep(28,3))
p2 = c(15,rep(18,2),rep(19,8),rep(20,15),rep(21,29),rep(22,48),
  rep(23,51),rep(24,83),rep(25,75),rep(26,55),rep(27,37),
  rep(28,21),rep(29,3),30)
```

As an example, please note that rep(14,4) means 14 days appears in the data 4 times, rep(15,5) means that the number 15 appears 5 times, etc.

Please address each of the following questions:

1. Present a frequency table for each distribution (10pts).
2. Identify the mode of each distribution (5pts).
3. Use arithmetic formulas (in R) to calculate the means for each prison; check your work using the mean() function (15pts).
4. Use arithmetic formulas (in R) to calculate the standard deviations for each prison; check your work using the sd() function. (15pts)
5. Calculate the range for each prison. (7pts)
6. Calculate the inter-quartile range (IQR) for each prison. (7pts)
7. Use the median() function to calculate the median waiting time in each prison; comment on which prison has the longer median waiting time (8pts).
8. Create a properly labeled side-by-side barplot for each distribution (15pts).
9. Create a properly labeled boxplot of the 2 distributions (15pts).
10. Your work should be assembled into a neatly formatted, readable, and well-organized pdf file so Jane and Jordan can easily read it (3 points).


### Lesson 13 - Tuesday 3/12/24

* Assigned reading: Chapter 6 (probability)
* Reminder: Assignment #2 due at 11:59pm on Monday 3/18/24; no questions after 5pm on Friday 3/15/24.
* Slides [linked here](https://github.com/rwb/ccjs200/blob/main/gfiles/lesson13s.pdf)
* *Note*: I took the coin flipping examples out of today's notes because I think it was confusing.

### Lesson 14 - Thursday 3/14/24

* Assigned reading: Chapter 6 (probability)
* Happy pi day!
* Reminder: Assignment #2 due at 11:59pm on Monday 3/18/24; no questions after 5pm on Friday 3/15/24.
* No class next week - spring break!
* Slides [linked here](https://github.com/rwb/ccjs200/blob/main/gfiles/lesson14.pdf)

### Lesson 15 - Tuesday 3/26/24

* Assigned reading: Chapter 6 (probability)
* Reminder: Exam 2 scheduled for Thursday 4/4/24.
* Slides [linked here](https://github.com/rwb/ccjs200/blob/main/gfiles/lesson15.pdf)

### Lesson 16 - Thursday 3/28/24

* Assigned reading: Chapter 6 (probability)
* Reminder: Exam 2 scheduled for Thursday 4/4/24.
* Slides [linked here](https://github.com/rwb/ccjs200/blob/main/gfiles/lesson16.pdf)

### Lesson 17 - Practice Problems for Exam #2 - Tuesday 4/2/24

*Note*: remember that exam 2 starts with Lesson 11 and Chapter 5 and ends with Chapter 6, page 181. Please focus your studying efforts on material we have covered in class and discussion sections (correct answers are below questions).

1. Consider the following dataset capturing causes of death among a state's prison inmate population for a 1-year period:

| Causes of Death | N = |
| :---------- | --:| 
| Cancer    | 337 |
| Heart Disease   | 278  |
| Stroke    | 189 |
| COVID-19 | 93 |
| Liver Failure | 81 |
| Other | 68 |
| Total | 1,046 |

1a. What is the variation ratio for this dataset?

```rout
a. 0.322
b. 0.187
c. 0.678
d. 0.588
e. 0.242
```
1b. What is the maximum value the variation ratio can take on in this dataset?

```rout
a. 0.917
b. 0.833
c. 0.723
d. 0.791
e. 0.642
```

2. Consider the following distribution of 10 parole risk scores (100 means highest risk):

```rout
78 80 84 82 86 87 71 86 96 84
```

2a. What is the range of the distribution?

```rout
a. 9
b. 17
c. 18
d. 28
e. 25
```

2b. What is the inter-quartile range of the distribution?

```rout
a. 6
b. 3
c. 2
d. 8
e. 7
```

2c. The sum of the mean deviations for this distribution is:

```rout
a. -2
b. -1
c. 0
d. 1
e. 2
```

2d. The sample variance of the distribution is:

```rout
a. 38.240
b. 42.489
c. 31.357
d. 6.518
e. 6.184
```

3. Consider the following data points for the number of days between prison release and first re-arrest for a sample of 9 ex-prisoners:

```rout
Data: 31 28 51 28 30 25 33 29 42
```

* What is the sample standard deviation of this dataset?

```Rout
a. 8.3
b. 16.5
c. 7.1
d. 9.7
e. 12.1
```

4. A group of juveniles were arrested during the month of January. We examine the prior arrest records for each of these juveniles and obtain the following information about their prior offending:

| Prior Arrests | Violent=No | Violent=Yes | Total|
| :---------- | --:| --:|--:| 
| Nonviolent = No    | 325 | 89 | 414 |
| Nonviolent = Yes   | 231 | 57 | 288 | 
| Total    | 556 | 146 | 702 |

a. What is the probability that someone drawn at random from this sample was not previously arrested? (general multiplication rule)

```Rout
a. 0.287
b. 0.382
c. 0.463
d. 0.224
e. 0.512
```

b. What is the probability that someone drawn at random from this sample was previously arrested for either a violent or a nonviolent offense? (general addition rule)

```Rout
a. 0.537
b. 0.609
c. 0.422
d. 0.223
e. 0.311
```

c. What is the probability that someone drawn at random from this sample was previously arrested for both a violent and a nonviolent offense (use general multiplication rule)?

```Rout
a. 0.242
b. 0.085
c. 0.038
d. 0.123
e. 0.081
```

d. What is the probability that someone drawn at random from this sample was previously arrested for both a violent and a nonviolent offense (use restricted multiplication rule)?

```Rout
a. 0.242
b. 0.085
c. 0.038
d. 0.123
e. 0.081
```

5. Past experience tells us that the probability a newly released prisoner will be arrested for a new crime within 3 years is 0.682. Based on this information, calculate a probability distribution for the number of people who will be arrested for a new crime out of 5 new prison releasees (over the next 3 years).

5a. What is the probability that exactly 3 of the 5 people will be rearrested within 3 years?

```rout
a. 0.284
b. 0.475
c. 0.500
d. 0.321
e. 0.415
```

5b. What is the probability that 3 or more people will be arrested within 3 years?

```rout
a. 0.813
b. 0.721
c. 0.685
d. 0.905
e. 0.894
```

5c. What is the probability that 0 people will be arrested within 3 years?

```rout
a. 0.054
b. 0.037
c. 0.003
d. 0.015
e. 0.894
```

6. The average age of juveniles arrested in a particular community is 15.5; the standard deviation of the average age is 1.7. What is the age of someone who is 2 standard deviations below the mean?

```rout
a. 13.8
b. 12.1
c. 13.0
d. 14.2
e. 15.5
```

7. Consider a group of prison inmates being releaased from prison. We know the age of each of these inmates the first time they were arrested. Based on our calculations, we find that the average age of first arrest was 23.2 years old with a standard deviation of 2.1.

7a. What is the z-score of someone who was first arrested at age 21?

```rout
a. -1.25
b. -1.57
c. -1.05
d. -1.85
e. -1.37
```

7b. What is the percentile rank of someone who was first arrested at age 21?

```rout
a. 85.3
b. 50.0
c. 38.0
d. 14.7
e. 5.2
```

7c. What is the probability that a prisoner drawn at random from the population of prisoners has a first arrest age greater than 25 years old?

```rout
a. 0.195
b. 0.187
c. 0.782
d. 0.805
```

8. Historically, late night car accidents in High Noon County have had a 0.5 probability of involving an impaired driver. However, 5 of the most recent 7 late night accidents have involved an impaired driver. Our goal is to determine whether the hypothesis that p = 0.5 should be rejected. If we decide that events in the central 0.872 region of the probability distribution will not be strong enough evidence to reject the hypothesis that p = 0.5 (meaning that events in either 0.064 end of the distribution will be strong enough evidence to reject), what decision should we make?

```rout
a. reject hypothesis that p = 0.5
b. fail to reject hypothesis that p = 0.5
c. neither a nor b; evidence is inconclusive
```

9. As noted in practice problem 8, historically, late night car accidents in High Noon County have had a 0.5 probability of involving an impaired driver. Suppose we want to predict the probability that at least 2 out of the next 3 late night car accidents will involve an impaired driver. The correct probability estimate would be:

```rout
a. 0.2
b. 0.3
c. 0.4
d. 0.5
e. 0.6
```

10. A total of 7 people are expected to be released from the local prison next month. This prison primarily houses older inmates and the historical rearrest recidivism rate for people exiting this prison (over a 3-year follow-up period) is about 30.0% or 0.300. What is the estimated probability that none of these 7 people will be rearrested over the next 3 years?

```rout
a. 0.082
b. 0.157
c. 0.382
d. 0.321
e. 0.242
```

11. Deductive research means:

```rout
a. data followed by theory
b. theory followed by data
c. neither a nor b is correct
```

12. If a distribution is skewed so that it has a short left-hand tail and a long-right hand tail, the median will be ____ than the mean.

```rout
a. greater
b. less
c. not enough information to tell
```

Answer Key: 1a(c); 1b(b); 2a(e); 2b(a); 2c(c); 2d(b); 3(a); 4a(c); 4b(a); 4c(e); 4d(b); 5a(d); 5b(a); 5c(c); 6(b); 7a(c); 7b(d); 7c(a); 8(b); 9(d); 10(a); 11(b), 12(b)

```rout
Worked solutions to practice problems:

1a. fm = 337, n = 1046; 1-fm/n = 1-337/1046 = 0.678
1b. max(vr) = 1-(n/k)/n = 1-(1046/6)/1046 = 0.833
2a. range(x) = max(x)-min(x) = 96-71 = 25

2b. IQR - inter-quartile range
    - step 1 - sort the data set: 71 78 80 82 84 84 86 86 87 96
    - step 2 - median position = 5.5 and truncated median position = 5
    - step 3 - identify q1 and q3 by evaluating (TMP+1)/2 = (5+1)/2 = 3
    - step 4 - q1 is 80 (3rd position from left); q3 is 86 (3rd position from right)
    - step 5 - iqr = 86-80 = 6

2c. sum of the mean deviations = sum(x-mean(x))
    - step 1 - list the values: 78 80 84 82 86 87 71 86 96 84
    - step 2 - calculate the mean: (78+80+84+82+86+87+71+86+96+84)/10 = 834/10 = 83.4
    - step 3 - calculate the deviations: -5.4 -3.4 0.6 -1.4 2.6 3.6 -12.4 2.6 12.6 0.6
    - step 4 - sum the deviations: 0

2d. sample variance = 1/(n-1) sum(x-xbar)^2
    - step 1 - 1/(n-1) = 1/(10-1) = 1/9 = 0.111
    - step 2 - mean = (78+80+84+82+86+87+71+86+96+84)/10 = 834/10 = 83.4
    - step 3 - (78-83.4)^2 =  29.16
               (80-83.4)^2 =  11.56
               (84-83.4)^2 =   0.36
               (82-83.4)^2 =   1.96
               (86-83.4)^2 =   6.76
               (87-83.4)^2 =  12.96
               (71-83.4)^2 = 153.76
               (86-83.4)^2 =   6.76
               (96-83.4)^2 = 158.76
               (84-83.4)^2 =   0.36
               sum of squares = 29.16+11.56+0.36+1.96+6.76+12.96+
                  153.76+6.76+158.76+0.36 = 382.4
    - step 4 - 0.111*382.4 = 42.446

3. standard deviation of: 31 28 51 28 30 25 33 29 42
   - step 1 - 1/(n-1) = 1/(9-1) = 1/8 = 0.125
   - step 2 - mean = (31+28+51+28+30+25+33+29+42)/9 = 33
   - step 3 - (31-33)^2 =    4
              (28-33)^2 =   25
              (51-33)^2 =  324
              (28-33)^2 =   25
              (30-33)^2 =    9
              (25-33)^2 =   64
              (33-33)^2 =    0
              (29-33)^2 =   16
              (42-33)^2 =   81
   - step 4 - sum the squares = 4+25+324+25+9+64+0+16+81 = 548
   - step 5 - calculate the variance: 0.125*548 = 68.5
   - step 6 - take the square root: sqrt(68.5) = 8.3

4a. p(violent = no & nonviolent = no); use general multiplication rule
    p(violent = no) x p(nonviolent = no | violent=no)
    556/702           325/556                        
    0.792             0.585      = 0.463

4b. p(violent or nonviolent); use general addition rule
    p(violent) + p(nonviolent) - p(violent and nonviolent) 
    146/702    + 288/702       - 57/702
    0.208      + 0.410         - 0.081 = 0.537

4c. p(violent and nonviolent); use general multiplication rule
    p(violent)*p(nonviolent|violent)
    146/702   * 57/146 =
    0.208     * 0.390 = 0.081

4d. p(violent and nonviolent); use restricted multiplication rule
    p(violent) * p(nonviolent)
    146/702      288/702
    0.208      * 0.410 = 0.085

5. calculate the probability distribution of x (from 0 to 5) when p = 0.682 and n is 5.

p = 0.682
1-p = 0.318
n = 5
x ranges from 0 to 5

p(x=0) = 5!/(0!(5-0)!) 0.682^0 (0.318)^(5-0)
       = 120/(1*120)*1*0.003
       = 0.003
p(x=1) = 5!/(1!(5-1)!) 0.682^1 (0.318)^(5-1)
       = 120/24 * 0.682 * 0.010
       = 5*0.682*0.010
       = 0.034
p(x=2) = 5!/(2!(5-2)!) 0.682^2 (0.318)^(5-2)
       = 120/12 * 0.465 * 0.032
       = 10*0.465*0.032
       = 0.149
p(x=3) = 5!/(3!(5-3)!) 0.682^3 (0.318)^(5-3)
       = 120/12 * 0.317 * 0.101
       = 10*0.317*0.101
       = 0.320
p(x=4) = 5!/(4!(5-4)!) 0.682^4 (0.318)^(5-4)
       = 120/24 * 0.216 * 0.318
       = 5*0.216*0.318
       = 0.343
p(x=5) = 5!/(5!(5-5)!) 0.682^5 (0.318)^(5-5)
       = 1 * 0.148 * 0.318
       = 1*0.148*1
       = 0.148    
              
5a. p(x=3) = 0.320
5b. p(x=3)+p(x=4)+p(x=5) = 0.320+0.343+0.148 = 0.811
5c. p(x=0) = 0.003

6. Age | 2 sd's below mean = mean(age)-2*sd(age) = 15.5-2*1.7 = 12.1

7. average age = 23.2; sd = 2.1

7a. z-score | age = 21 = (21-23.2)/2.1 = -1.05
7b. percentile rank of someone who is 21 years old
    z-table entry for a z-score of -1.05 = 0.3531
    percentile = 0.5-0.3531 = 0.147 or 14.7th percentile (see figure below)
7c. percentile rank of someone who is 25 years old
    z-score | age = 25 = (25-23.2)/2.1 = 0.857
    look up z-table for zscore = 0.86, entry is 0.3051
    percentile = 0.5+0.3051 = 0.8051 or 80.5th percentile
    p(draw someone at random above 80.5th percentile) = 1-0.805 = 0.195

8. test hypothesis that p = 0.5; Information given: n=7, x=0,1,2,3,4,5,6,7; hypothesized p = 0.5

p(x=0|p=0.5) = 7!(0!(7-0)!)*0.5^0*(1-0.5)^(7-0)
             = 1*1*0.5^7
             = 0.008   ---> critical
p(x=1|p=0.5) = 7!(1!(7-1)!)*0.5^1*(1-0.5)^(7-1)
             = 7*0.5*0.016
             = 0.056   ---> critical
p(x=2|p=0.5) = 7!(2!(7-2)!)*0.5^2*(1-0.5)^(7-2)
             = 21*0.25*0.031
             = 0.163
p(x=3|p=0.5) = 7!(3!(7-3)!)*0.5^3*(1-0.5)^(7-3)
             = 35*0.125*0.063
             = 0.276
p(x=4|p=0.5) = 7!(4!(7-4)!)*0.5^4*(1-0.5)^(7-4)
             = 35*0.063*0.125
             = 0.276
p(x=5|p=0.5) = 7!(5!(7-5)!)*0.5^5*(1-0.5)^(7-5)
             = 21*0.031*0.25
             = 0.163
p(x=6|p=0.5) = 7!(6!(7-6)!)*0.5^6*(1-0.5)^(7-6)
             = 7*0.016*0.5
             = 0.056   ----> critical
p(x=7|p=0.5) = 7!(7!(7-7)!)*0.5^7*(1-0.5)^(7-7)
             = 1*0.008*1
             = 0.008   ----> critical

decision: fail to reject

9. p = 0.5, n = 3, x = 0,1,2,3; 
p(at least 2 out of the next 3 accidents will involve an impaired driver)

p(x=0|p=0.5) = 3!(0!(3-0)!)*0.5^0*(1-0.5)^(3-0)
             = 1*1*0.125
             = 0.125
p(x=1|p=0.5) = 3!(1!(3-1)!)*0.5^1*(1-0.5)^(3-1)
             = 3*0.5*0.25
             = 0.375
p(x=2|p=0.5) = 3!(2!(3-2)!)*0.5^2*(1-0.5)^(3-2)
             = 3*0.25*0.5
             = 0.375
p(x=3|p=0.5) = 3!(3!(3-0)!)*0.5^3*(1-0.5)^(3-3)
             = 1*0.125*1
             = 0.125

p(at least 2) = p(x=2)+p(x=3) = 0.375+0.125 = 0.5

10. n = 7, p = 0.3, x=0

p(x=0|p=0.3) = 7!(0!(7-0)!)*0.3^0*(1-0.3)^(7-0)
             = 1*1*0.082
             = 0.082

11. b - theory followed by data
12. median less than mean when distribution has a long right hand tail
```

<p align="center">
<img src="/gfiles/f1a.png" width="700px">
</p>


### Lesson 18 - Tuesday 4/9/24

*Note*: final exam starts with today's material (we are at page 166 in the book)

* We now consider several examples of binomial hypothesis testing using R.
* Example 1: worked lojack example from pp. 166-172 of the textbook (one-tailed test).

```r
# hypothesis to be tested is theta = 0.4

theta = 0.4

# specify probability distribution of 
# outcome count if we study 10 cars
# and theta is fixed at 0.4

n = 10
x = seq(from=0,to=10,by=1)
px = choose(n,x)*theta^x*(1-theta)^(n-x)
data.frame(x,px)

# example in book, pp. 166-172 is a
# one-tailed or "directional" test
# we only construe evidence in the upper
# end of the distribution as contradicting
# the hypothesis that theta = 0.4

# what evidence would convince us to reject
# the hypothesis that theta = 0.4
# against the alternaative that theta > 0.4
# set our p-value to be 0.05
# p-value = p(reject hypothesis | hypothesis is right) < 0.05
# p-value = p(result at least as extreme as obtained | hypothesis is right) < 0.05

# add up probabilities at upper end of distribution
# such that the sum is less than 0.05

# these are the p-values

px[11]
px[10]+px[11]
px[9]+px[10]+px[11]
px[8]+px[9]+px[10]+px[11]

# based on these calculations we see that the events
# in the sample space x={8,9,10} form the critical region

decision <- ifelse(x>=8,"reject","fail to reject")
data.frame(x,px,decision)

# what do our data say?
# the data say that 8 out of 10 cars with lojack
# were successfully recovered.
# this result falls in the critical region so we
# reject the hypothesis that theta = 0.4 in favor
# of the hypothesis that theta > 0.4
# hypothesis that theta > 0.4 is more consistent 
# with the observed data
```

* Here is our output:

```rout
> # hypothesis to be tested is theta = 0.4
> 
> theta = 0.4
> 
> # specify probability distribution of 
> # outcome count if we study 10 cars
> # and theta is fixed at 0.4
> 
> n = 10
> x = seq(from=0,to=10,by=1)
> px = choose(n,x)*theta^x*(1-theta)^(n-x)
> data.frame(x,px)
    x           px
1   0 0.0060466176
2   1 0.0403107840
3   2 0.1209323520
4   3 0.2149908480
5   4 0.2508226560
6   5 0.2006581248
7   6 0.1114767360
8   7 0.0424673280
9   8 0.0106168320
10  9 0.0015728640
11 10 0.0001048576
> 
> # example in book, pp. 166-172 is a
> # one-tailed or "directional" test
> # we only construe evidence in the upper
> # end of the distribution as contradicting
> # the hypothesis that theta = 0.4
> 
> # what evidence would convince us to reject
> # the hypothesis that theta = 0.4
> # against the alternaative that theta > 0.4
> # set our p-value to be 0.05
> # p-value = p(reject hypothesis | hypothesis is right) < 0.05
> # p-value = p(result at least as extreme as obtained | hypothesis is right) < 0.05
> 
> # add up probabilities at upper end of distribution
> # such that the sum is less than 0.05
> 
> # these are the p-values
> 
> px[11]
[1] 0.0001048576
> px[10]+px[11]
[1] 0.001677722
> px[9]+px[10]+px[11]
[1] 0.01229455
> px[8]+px[9]+px[10]+px[11]
[1] 0.05476188
> 
> # based on these calculations we see that the events
> # in the sample space x={8,9,10} form the critical region
> 
> decision <- ifelse(x>=8,"reject","fail to reject")
> data.frame(x,px,decision)
    x           px       decision
1   0 0.0060466176 fail to reject
2   1 0.0403107840 fail to reject
3   2 0.1209323520 fail to reject
4   3 0.2149908480 fail to reject
5   4 0.2508226560 fail to reject
6   5 0.2006581248 fail to reject
7   6 0.1114767360 fail to reject
8   7 0.0424673280 fail to reject
9   8 0.0106168320         reject
10  9 0.0015728640         reject
11 10 0.0001048576         reject
> 
> # what do our data say?
> # the data say that 8 out of 10 cars with lojack
> # were successfully recovered.
> # this result falls in the critical region so we
> # reject the hypothesis that theta = 0.4 in favor
> # of the hypothesis that theta > 0.4
> # hypothesis that theta > 0.4 is more consistent 
> # with the observed data
>
```

* Example 2: we study 30 cities at two time points. The data collected is the homicide rate at each time point.

```r
########################################################
# example 2: 30 largest cities were measured on their 
# homicide rate at two different times
# 22 of the cities increased and 8 cities declined
# test hypothesis that theta = 0.5 (two-tailed)
# against the alternative that theta not equal to 0.5
# p < 0.05 significance level

options(scipen=100)
theta = 0.5

# specify probability distribution of 
# outcome count if we study 30 cities
# and theta is fixed at 0.5

n = 30
x = seq(from=0,to=30,by=1)
px = choose(n,x)*theta^x*(1-theta)^(n-x)
data.frame(x,px)

# add up probabilities at both ends of the distribution
# such that the sum at each end is less than 0.025

# lower tail

px[1]
sum(px[1:2])
sum(px[1:3])
sum(px[1:4])
sum(px[1:5])
sum(px[1:6])
sum(px[1:7])
sum(px[1:8])
sum(px[1:9])
sum(px[1:10])
sum(px[1:11])

# upper tail

px[31]
sum(px[30:31])
sum(px[29:31])
sum(px[28:31])
sum(px[27:31])
sum(px[26:31])
sum(px[25:31])
sum(px[24:31])
sum(px[23:31])
sum(px[22:31])
sum(px[21:31])

# based on these calculations we see that the events
# x={0:9,21:30} form the critical region

decision <- ifelse(x %in% 0:9 | x %in% 21:30,"reject","fail to reject")
data.frame(x,px,decision)

# what do our data say?
# the data say that 22 out of 30 cities experienced
# an increase in their homicide rates from time 1 to time 2
# it is unlikely we would get a result at least this extreme
# if theta were equal to 0.5
```

* Here is the output for Example 2:

```rout
> ########################################################
> # example 2: 30 largest cities were measured on their 
> # homicide rate at two different times
> # 22 of the cities increased and 8 cities declined
> # test hypothesis that theta = 0.5 (two-tailed)
> # against the alternative that theta not equal to 0.5
> # p < 0.05 significance level
> 
> options(scipen=100)
> theta = 0.5
> 
> # specify probability distribution of 
> # outcome count if we study 30 cities
> # and theta is fixed at 0.5
> 
> n = 30
> x = seq(from=0,to=30,by=1)
> px = choose(n,x)*theta^x*(1-theta)^(n-x)
> data.frame(x,px)
    x                 px
1   0 0.0000000009313226
2   1 0.0000000279396772
3   2 0.0000004051253200
4   3 0.0000037811696529
5   4 0.0000255228951573
6   5 0.0001327190548182
7   6 0.0005529960617423
8   7 0.0018959864974022
9   8 0.0054509611800313
10  9 0.0133245717734098
11 10 0.0279816007241607
12 11 0.0508756376802921
13 12 0.0805530929937959
14 13 0.1115350518375635
15 14 0.1354354200884700
16 15 0.1444644480943680
17 16 0.1354354200884700
18 17 0.1115350518375635
19 18 0.0805530929937959
20 19 0.0508756376802921
21 20 0.0279816007241607
22 21 0.0133245717734098
23 22 0.0054509611800313
24 23 0.0018959864974022
25 24 0.0005529960617423
26 25 0.0001327190548182
27 26 0.0000255228951573
28 27 0.0000037811696529
29 28 0.0000004051253200
30 29 0.0000000279396772
31 30 0.0000000009313226
> 
> # add up probabilities at both ends of the distribution
> # such that the sum at each end is less than 0.025
> 
> # lower tail
> 
> px[1]
[1] 0.0000000009313226
> sum(px[1:2])
[1] 0.000000028871
> sum(px[1:3])
[1] 0.0000004339963
> sum(px[1:4])
[1] 0.000004215166
> sum(px[1:5])
[1] 0.00002973806
> sum(px[1:6])
[1] 0.0001624571
> sum(px[1:7])
[1] 0.0007154532
> sum(px[1:8])
[1] 0.00261144
> sum(px[1:9])
[1] 0.008062401
> sum(px[1:10])
[1] 0.02138697
> sum(px[1:11])
[1] 0.04936857
> 
> # upper tail
> 
> px[31]
[1] 0.0000000009313226
> sum(px[30:31])
[1] 0.000000028871
> sum(px[29:31])
[1] 0.0000004339963
> sum(px[28:31])
[1] 0.000004215166
> sum(px[27:31])
[1] 0.00002973806
> sum(px[26:31])
[1] 0.0001624571
> sum(px[25:31])
[1] 0.0007154532
> sum(px[24:31])
[1] 0.00261144
> sum(px[23:31])
[1] 0.008062401
> sum(px[22:31])
[1] 0.02138697
> sum(px[21:31])
[1] 0.04936857
> 
> # based on these calculations we see that the events
> # x={0:9,21:30} form the critical region
> 
> decision <- ifelse(x %in% 0:9 | x %in% 21:30,"reject","fail to reject")
> data.frame(x,px,decision)
    x                 px       decision
1   0 0.0000000009313226         reject
2   1 0.0000000279396772         reject
3   2 0.0000004051253200         reject
4   3 0.0000037811696529         reject
5   4 0.0000255228951573         reject
6   5 0.0001327190548182         reject
7   6 0.0005529960617423         reject
8   7 0.0018959864974022         reject
9   8 0.0054509611800313         reject
10  9 0.0133245717734098         reject
11 10 0.0279816007241607 fail to reject
12 11 0.0508756376802921 fail to reject
13 12 0.0805530929937959 fail to reject
14 13 0.1115350518375635 fail to reject
15 14 0.1354354200884700 fail to reject
16 15 0.1444644480943680 fail to reject
17 16 0.1354354200884700 fail to reject
18 17 0.1115350518375635 fail to reject
19 18 0.0805530929937959 fail to reject
20 19 0.0508756376802921 fail to reject
21 20 0.0279816007241607 fail to reject
22 21 0.0133245717734098         reject
23 22 0.0054509611800313         reject
24 23 0.0018959864974022         reject
25 24 0.0005529960617423         reject
26 25 0.0001327190548182         reject
27 26 0.0000255228951573         reject
28 27 0.0000037811696529         reject
29 28 0.0000004051253200         reject
30 29 0.0000000279396772         reject
31 30 0.0000000009313226         reject
> 
> # what do our data say?
> # the data say that 22 out of 30 cities experienced
> # an increase in their homicide rates from time 1 to time 2
> # it is unlikely we would get a result at least this extreme
> # if theta were equal to 0.5
>
```

* Example 3: hot spot patrols and crime reduction

```r
########################################################
# example 3: 17 hot spots receive intensive patrol
# calls for service dropped in 12 of them after patrol
# test hypothesis that theta = 0.5 (one-tailed)
# against alternative that theta > 0.5
# p < 0.05 significance level

# suppress scientific notation

options(scipen=100)

# set theta to 0.5

theta = 0.5

# specify probability distribution of 
# outcome count if we study 17 hot spots
# and theta is fixed at 0.5

n = 17
x = seq(from=0,to=17,by=1)
px = choose(n,x)*theta^x*(1-theta)^(n-x)
data.frame(x,px)

# add up probabilities at upper end of the distribution
# such that the sum at the upper end is less than 0.050

# upper tail

px[18]
sum(px[17:18])
sum(px[16:18])
sum(px[15:18])
sum(px[14:18])
sum(px[13:18])
sum(px[12:18])

# based on these calculations we see that the events
# x={13,14,15,16,17} form the critical region

decision <- ifelse(x %in% 13:17,"reject","fail to reject")
data.frame(x,px,decision)

# what do our data say?
# the data say that 12 out of 17 hot spots experienced
# a decline after patrols
# we fail to reject hypothesis that theta = 0.5
# in other words, hypothesis that theta = 0.5 is more
# consistent with the data than alternative hypothesis that
# theta > 0.5

# let's do a simulation to demonstrate the p-value for this test is right

# use y>=13 as critical region

reject = vector()
for(i in 1:1000000){
  y = rbinom(n=1,size=17,p=0.5)
  reject[i] = ifelse(y>=13,"reject","fail to reject")
  }

table(reject)/length(reject)
```

* Here is the output for Example 3:

```rout
> ########################################################
> # example 3: 17 hot spots receive intensive patrol
> # calls for service dropped in 12 of them after patrol
> # test hypothesis that theta = 0.5 (one-tailed)
> # against alternative that theta > 0.5
> # p < 0.05 significance level
> 
> # suppress scientific notation
> 
> options(scipen=100)
> 
> # set theta to 0.5
> 
> theta = 0.5
> 
> # specify probability distribution of 
> # outcome count if we study 17 hot spots
> # and theta is fixed at 0.5
> 
> n = 17
> x = seq(from=0,to=17,by=1)
> px = choose(n,x)*theta^x*(1-theta)^(n-x)
> data.frame(x,px)
    x             px
1   0 0.000007629395
2   1 0.000129699707
3   2 0.001037597656
4   3 0.005187988281
5   4 0.018157958984
6   5 0.047210693359
7   6 0.094421386719
8   7 0.148376464844
9   8 0.185470581055
10  9 0.185470581055
11 10 0.148376464844
12 11 0.094421386719
13 12 0.047210693359
14 13 0.018157958984
15 14 0.005187988281
16 15 0.001037597656
17 16 0.000129699707
18 17 0.000007629395
> 
> # add up probabilities at upper end of the distribution
> # such that the sum at the upper end is less than 0.050
> 
> # upper tail
> 
> px[18]
[1] 0.000007629395
> sum(px[17:18])
[1] 0.0001373291
> sum(px[16:18])
[1] 0.001174927
> sum(px[15:18])
[1] 0.006362915
> sum(px[14:18])
[1] 0.02452087
> sum(px[13:18])
[1] 0.07173157
> sum(px[12:18])
[1] 0.166153
> 
> # based on these calculations we see that the events
> # x={13,14,15,16,17} form the critical region
> 
> decision <- ifelse(x %in% 13:17,"reject","fail to reject")
> data.frame(x,px,decision)
    x             px       decision
1   0 0.000007629395 fail to reject
2   1 0.000129699707 fail to reject
3   2 0.001037597656 fail to reject
4   3 0.005187988281 fail to reject
5   4 0.018157958984 fail to reject
6   5 0.047210693359 fail to reject
7   6 0.094421386719 fail to reject
8   7 0.148376464844 fail to reject
9   8 0.185470581055 fail to reject
10  9 0.185470581055 fail to reject
11 10 0.148376464844 fail to reject
12 11 0.094421386719 fail to reject
13 12 0.047210693359 fail to reject
14 13 0.018157958984         reject
15 14 0.005187988281         reject
16 15 0.001037597656         reject
17 16 0.000129699707         reject
18 17 0.000007629395         reject
> 
> # what do our data say?
> # the data say that 12 out of 17 hot spots experienced
> # a decline after patrols
> # we fail to reject hypothesis that theta = 0.5
> # in other words, hypothesis that theta = 0.5 is more
> # consistent with the data than alternative hypothesis that
> # theta > 0.5
> 
> # let's do a simulation to demonstrate the p-value for this test is right
> 
> # use y>=13 as critical region
> 
> reject = vector()
> for(i in 1:1000000){
+   y = rbinom(n=1,size=17,p=0.5)
+   reject[i] = ifelse(y>=13,"reject","fail to reject")
+   }
> 
> table(reject)/length(reject)
reject
fail to reject         reject 
      0.975722       0.024278 
>
```

* Example 4: Prison treatment program and recidivism

```R
########################################################
# example 4: 12 people released from prison after
# receiving a treatment program. 
# historical recidivism rate = 0.6
# 4 out of the 12 were observed to recidivate
# test hypothesis that theta = 0.6 (one-tailed)
# against alternative that theta < 0.6
# p < 0.05 significance level

# suppress scientific notation

options(scipen=100)

# set theta to 0.6

theta = 0.6

# specify probability distribution of 
# outcome count if we study 12 people
# and theta is fixed at 0.6

n = 12
x = seq(from=0,to=12,by=1)
px = choose(n,x)*theta^x*(1-theta)^(n-x)
data.frame(x,px)

# add up probabilities at lower end of the distribution
# such that the sum is less than 0.05

# lower tail

px[1]
sum(px[1:2])
sum(px[1:3])
sum(px[1:4])
sum(px[1:5])

# based on these calculations we see that the events
# x={0,1,2,3} form the critical region

decision <- ifelse(x %in% 0:3,"reject","fail to reject")
data.frame(x,px,decision)

# what do our data say?
# the data say that 4 out of 12 people recidivated
# it is plausible we could get a result at least this extreme
# if theta were equal to 0.6
# decision: fail to reject hypothesis that theta = 0.6
```

* Here is our output for Example 4:

```Rout
> ########################################################
> # example 4: 12 people released from prison after
> # receiving a treatment program. 
> # historical recidivism rate = 0.6
> # 4 out of the 12 were observed to recidivate
> # test hypothesis that theta = 0.6 (one-tailed)
> # against alternative that theta < 0.6
> # p < 0.05 significance level
> 
> # suppress scientific notation
> 
> options(scipen=100)
> 
> # set theta to 0.6
> 
> theta = 0.6
> 
> # specify probability distribution of 
> # outcome count if we study 12 people
> # and theta is fixed at 0.6
> 
> n = 12
> x = seq(from=0,to=12,by=1)
> px = choose(n,x)*theta^x*(1-theta)^(n-x)
> data.frame(x,px)
    x            px
1   0 0.00001677722
2   1 0.00030198989
3   2 0.00249141658
4   3 0.01245708288
5   4 0.04204265472
6   5 0.10090237133
7   6 0.17657914982
8   7 0.22703033549
9   8 0.21284093952
10  9 0.14189395968
11 10 0.06385228186
12 11 0.01741425869
13 12 0.00217678234
> 
> # add up probabilities at lower end of the distribution
> # such that the sum is less than 0.05
> 
> # lower tail
> 
> px[1]
[1] 0.00001677722
> sum(px[1:2])
[1] 0.0003187671
> sum(px[1:3])
[1] 0.002810184
> sum(px[1:4])
[1] 0.01526727
> sum(px[1:5])
[1] 0.05730992
> 
> # based on these calculations we see that the events
> # x={0,1,2,3} form the critical region
> 
> decision <- ifelse(x %in% 0:3,"reject","fail to reject")
> data.frame(x,px,decision)
    x            px       decision
1   0 0.00001677722         reject
2   1 0.00030198989         reject
3   2 0.00249141658         reject
4   3 0.01245708288         reject
5   4 0.04204265472 fail to reject
6   5 0.10090237133 fail to reject
7   6 0.17657914982 fail to reject
8   7 0.22703033549 fail to reject
9   8 0.21284093952 fail to reject
10  9 0.14189395968 fail to reject
11 10 0.06385228186 fail to reject
12 11 0.01741425869 fail to reject
13 12 0.00217678234 fail to reject
> 
> # what do our data say?
> # the data say that 4 out of 12 people recidivated
> # it is plausible we could get a result at least this extreme
> # if theta were equal to 0.6
> # decision: fail to reject hypothesis that theta = 0.6
>
```

* Example 5: updated business cycle analysis

```r
########################################################
# example 5: 13 business cycles
# in 10 of the 13 cycles, robbery increased (relative to
# its changes during the growth phase) when the economy
# tipped into a recession
# if economy were unrelated to crime we would expect
# that p(robbery increases | recession) = 0.5
# conduct a two-tailed test at p < .15 significance level

# suppress scientific notation

options(scipen=100)

# set theta to 0.5

theta = 0.5

# specify probability distribution of 
# outcome count if we study 12 people
# and theta is fixed at 0.6

n = 13
x = seq(from=0,to=13,by=1)
px = choose(n,x)*theta^x*(1-theta)^(n-x)
data.frame(x,px)

# add up probabilities at lower end of the distribution
# such that the sum is less than 0.075

# lower tail

px[1]
sum(px[1:2])
sum(px[1:3])
sum(px[1:4])
sum(px[1:5])

# add up probabilities at upper end of the distribution
# such that the sum is less than 0.075

# upper tail

px[14]
sum(px[13:14])
sum(px[12:14])
sum(px[11:14])
sum(px[10:14])

# based on these calculations we see that the events
# x={0,1,2,3,10,11,12,13} form the critical region

decision <- ifelse(x %in% 0:3 | x %in% 10:13,"reject","fail to reject")
data.frame(x,px,decision)

# what do our data say?
# the data say that robbery increased at the end of 10 out of 13 
# business cycles (i.e., when the economy tips into recession)
# decision: we reject hypothesis that theta = 0.5
# in favor of hypothesis that theta is not equal to 0.5
```

* Here is the output for Example 5.

```rout
> ########################################################
> # example 5: 13 business cycles
> # in 10 of the 13 cycles, robbery increased (relative to
> # its changes during the growth phase) when the economy
> # tipped into a recession
> # if economy were unrelated to crime we would expect
> # that p(robbery increases | recession) = 0.5
> # conduct a two-tailed test at p < .15 significance level
> 
> # suppress scientific notation
> 
> options(scipen=100)
> 
> # set theta to 0.5
> 
> theta = 0.5
> 
> # specify probability distribution of 
> # outcome count if we study 12 people
> # and theta is fixed at 0.6
> 
> n = 13
> x = seq(from=0,to=13,by=1)
> px = choose(n,x)*theta^x*(1-theta)^(n-x)
> data.frame(x,px)
    x           px
1   0 0.0001220703
2   1 0.0015869141
3   2 0.0095214844
4   3 0.0349121094
5   4 0.0872802734
6   5 0.1571044922
7   6 0.2094726562
8   7 0.2094726562
9   8 0.1571044922
10  9 0.0872802734
11 10 0.0349121094
12 11 0.0095214844
13 12 0.0015869141
14 13 0.0001220703
> 
> # add up probabilities at lower end of the distribution
> # such that the sum is less than 0.075
> 
> # lower tail
> 
> px[1]
[1] 0.0001220703
> sum(px[1:2])
[1] 0.001708984
> sum(px[1:3])
[1] 0.01123047
> sum(px[1:4])
[1] 0.04614258
> sum(px[1:5])
[1] 0.1334229
> 
> # add up probabilities at upper end of the distribution
> # such that the sum is less than 0.075
> 
> # upper tail
> 
> px[14]
[1] 0.0001220703
> sum(px[13:14])
[1] 0.001708984
> sum(px[12:14])
[1] 0.01123047
> sum(px[11:14])
[1] 0.04614258
> sum(px[10:14])
[1] 0.1334229
> 
> # based on these calculations we see that the events
> # x={0,1,2,3,10,11,12,13} form the critical region
> 
> decision <- ifelse(x %in% 0:3 | x %in% 10:13,"reject","fail to reject")
> data.frame(x,px,decision)
    x           px       decision
1   0 0.0001220703         reject
2   1 0.0015869141         reject
3   2 0.0095214844         reject
4   3 0.0349121094         reject
5   4 0.0872802734 fail to reject
6   5 0.1571044922 fail to reject
7   6 0.2094726562 fail to reject
8   7 0.2094726562 fail to reject
9   8 0.1571044922 fail to reject
10  9 0.0872802734 fail to reject
11 10 0.0349121094         reject
12 11 0.0095214844         reject
13 12 0.0015869141         reject
14 13 0.0001220703         reject
> 
> # what do our data say?
> # the data say that robbery increased at the end of 10 out of 13 
> # business cycles (i.e., when the economy tips into recession)
> # decision: we reject hypothesis that theta = 0.5
> # in favor of hypothesis that theta is not equal to 0.5
>
```

* Example 6: standard normal variable

```r
#######################################################
# example 6: x is a standard normal variable
# this means it is normally distributed with a 
# mean of zero and a standard deviation of one
# what is the probability that a case drawn at random
# from this distribution is greater than zero?

x = rnorm(n=10000000,mean=0,sd=1)

x0 = ifelse(x>0,"greater than zero","less than zero")
table(x0)/length(x0)
```

* Here is the output:

```rout
> #######################################################
> # example 6: x is a standard normal variable
> # this means it is normally distributed with a 
> # mean of zero and a standard deviation of one
> # what is the probability that a case drawn at random
> # from this distribution is greater than zero
> 
> x = rnorm(n=10000000,mean=0,sd=1)
> 
> x0 = ifelse(x>0,"greater than zero","less than zero")
> table(x0)/length(x0)
x0
greater than zero    less than zero 
        0.5002064         0.4997936 
>
```

* Example 7: measuring the probability that a case drawn from a standard normal distribution will be between 0 and 1.5.

```r
#######################################################
# example 7: x is a standard normal variable
# this means it is normally distributed with a 
# mean of zero and a standard deviation of one
# what is the probability that a case drawn at random
# from this distribution lies between 0 and 1.5
# compare results to z-table on page 533

x = rnorm(n=10000000,mean=0,sd=1)

xint = ifelse(x>0 & x<1.5,"between 0 and 1.5","other")
table(xint)/length(xint)
```

* Here is our output:
  
```rout
> #######################################################
> # example 7: x is a standard normal variable
> # this means it is normally distributed with a 
> # mean of zero and a standard deviation of one
> # what is the probability that a case drawn at random
> # from this distribution lies between 0 and 1.5
> # compare results to z-table on page 533
> 
> x = rnorm(n=10000000,mean=0,sd=1)
> 
> xint = ifelse(x>0 & x<1.5,"between 0 and 1.5","other")
> table(xint)/length(xint)
xint
between 0 and 1.5             other 
        0.4334062         0.5665938
```

* Example 8: waiting time between verdict and sentencing.

```r
#######################################################
# example 8: let x be the waiting time (in days) between 
# verdict and sentencing in a state's criminal court system
# based on long-term historical data, we know the 
# average waiting time is normally distributed with a
# mean of 52 and a standard deviation of 8.

# now use the standard normal distribution to calculate 
# the percentile rank of someone who waits 62 days to be 
# sentenced after receiving their verdict


z = (62-52)/8
z

x = rnorm(n=10000000,mean=0,sd=1)
xpct = ifelse(x<z,"x<z","x>z")
table(xpct)/length(xpct)

# compare results to table on page 533
```

* Here is the output for Example 8:

```rout
> #######################################################
> # example 8: let x be the waiting time (in days) between 
> # verdict and sentencing in a state's criminal court system
> # based on long-term historical data, we know the 
> # average waiting time is normally distributed with a
> # mean of 52 and a standard deviation of 8.
> 
> # now use the standard normal distribution to calculate 
> # the percentile rank of someone who waits 62 days to be 
> # sentenced after receiving their verdict
> 
> 
> z = (62-52)/8
> z
[1] 1.25
> 
> x = rnorm(n=10000000,mean=0,sd=1)
> xpct = ifelse(x<z,"x<z","x>z")
> table(xpct)/length(xpct)
xpct
      x<z       x>z 
0.8943224 0.1056776 
> 
```
